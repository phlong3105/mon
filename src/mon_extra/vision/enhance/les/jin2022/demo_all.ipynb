{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG', '.jpeg', '.bmp'])\n",
    "\n",
    "def rgb_loader(img_path):\n",
    "    assert(is_image_file(img_path)==True)\n",
    "    return Image.open(img_path).convert('RGB')\n",
    "\n",
    "def gray_loader(img_path):\n",
    "    assert(is_image_file(img_path)==True)\n",
    "    return Image.open(img_path).convert('L')\n",
    "\n",
    "class loadImgs(data.Dataset):\n",
    "    def __init__(self, \n",
    "                 args, \n",
    "                 imgin_list, \n",
    "                 mode='demo'):\n",
    "        self.imgin_list  = imgin_list\n",
    "        self.args        = args\n",
    "        self.mode        = mode\n",
    "        \n",
    "        if self.args.use_gray:\n",
    "            self.img_loader  = gray_loader\n",
    "        else:\n",
    "            self.img_loader  = rgb_loader\n",
    "\n",
    "        self.data_list   = {'img_in': []}\n",
    "        random.seed(141)\n",
    "        for num_img in range(len(self.imgin_list)):\n",
    "            self.data_list['img_in'].append(self.imgin_list[num_img])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_in_path = self.data_list['img_in'][index]\n",
    "\n",
    "        if self.mode == 'demo':\n",
    "            img_in  = self.img_loader(img_in_path)\n",
    "            if self.args.load_size != 'None':\n",
    "                w, h      = img_in.size\n",
    "                img_in    = img_in.resize((512, 512))\n",
    "            if self.args.crop_size != 'None':\n",
    "                w, h      = img_in.size\n",
    "                crop_size = self.args.crop_size.strip('[]').split(', ')\n",
    "                crop_size = [int(item) for item in crop_size]\n",
    "                th, tw    = crop_size[0], crop_size[1]\n",
    "                x1        = random.randint(0, w - tw)\n",
    "                y1        = random.randint(0, h - th)\n",
    "                img_in    = img_in.crop((x1, y1, x1 + tw, y1 + th))\n",
    "        elif self.mode == 'val':\n",
    "            raise NotImplementedError\n",
    "        elif self.mode == 'predict':\n",
    "            img_in  = self.img_loader(img_in_path)\n",
    "        else:\n",
    "            print('Unrecognized mode! Please select among: (demo, val, predict)')\n",
    "            raise NotImplementedError\n",
    "\n",
    "        t_list = [transforms.ToTensor()]\n",
    "        composed_transform  = transforms.Compose(t_list)\n",
    "        if self.mode == 'demo':\n",
    "            img_in = composed_transform(img_in)\n",
    "        if self.mode == 'val':\n",
    "            raise NotImplementedError\n",
    "        if self.mode == 'predict':\n",
    "            img_in = composed_transform(img_in)\n",
    "\n",
    "        if self.mode == 'demo':\n",
    "            inputs = {'img_in': img_in}\n",
    "            return inputs\n",
    "        if self.mode == 'predict':\n",
    "            inputs = {'img_in': img_in}\n",
    "    def __len__(self):\n",
    "        return len(self.data_list['img_in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T08:40:20.592226Z",
     "start_time": "2023-12-07T08:40:14.493921Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 149\u001B[0m\n\u001B[1;32m    146\u001B[0m torch\u001B[38;5;241m.\u001B[39mmanual_seed(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    148\u001B[0m args\u001B[38;5;241m.\u001B[39mimgs_dir \u001B[38;5;241m=\u001B[39m args\u001B[38;5;241m.\u001B[39mout_dir\n\u001B[0;32m--> 149\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(args\u001B[38;5;241m.\u001B[39mimgs_dir):\n\u001B[1;32m    150\u001B[0m     os\u001B[38;5;241m.\u001B[39mmakedirs(args\u001B[38;5;241m.\u001B[39mimgs_dir)\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m args\u001B[38;5;241m.\u001B[39muse_gray:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from os import listdir\n",
    "from os.path import join, basename\n",
    "from torchvision import utils as vutils\n",
    "import load_data as DA\n",
    "from net import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MeanShift(nn.Conv2d):\n",
    "    def __init__(self, data_mean, data_std, data_range=1, norm=True):\n",
    "        c = len(data_mean)\n",
    "        super(MeanShift, self).__init__(c, c, kernel_size=1)\n",
    "        std = torch.Tensor(data_std)\n",
    "        self.weight.data = torch.eye(c).view(c, c, 1, 1)\n",
    "        if norm:\n",
    "            self.weight.data.div_(std.view(c, 1, 1, 1))\n",
    "            self.bias.data = -1 * data_range * torch.Tensor(data_mean)\n",
    "            self.bias.data.div_(std)\n",
    "        else:\n",
    "            self.weight.data.mul_(std.view(c, 1, 1, 1))\n",
    "            self.bias.data = data_range * torch.Tensor(data_mean)\n",
    "        self.requires_grad = False\n",
    "        \n",
    "class ExclusionLoss(nn.Module):\n",
    "    def __init__(self, level=3):\n",
    "        super(ExclusionLoss, self).__init__()\n",
    "        self.level = level\n",
    "        self.avg_pool = torch.nn.AvgPool2d(2, stride=2).type(torch.cuda.FloatTensor)\n",
    "        self.sigmoid = nn.Sigmoid().type(torch.cuda.FloatTensor)\n",
    "\n",
    "    def get_gradients(self, img1, img2):\n",
    "        gradx_loss = []\n",
    "        grady_loss = []\n",
    "\n",
    "        for l in range(self.level):\n",
    "            gradx1, grady1 = self.compute_gradient(img1)\n",
    "            gradx2, grady2 = self.compute_gradient(img2)\n",
    "            alphay = 1\n",
    "            alphax = 1\n",
    "            gradx1_s = (self.sigmoid(gradx1) * 2) - 1\n",
    "            grady1_s = (self.sigmoid(grady1) * 2) - 1\n",
    "            gradx2_s = (self.sigmoid(gradx2 * alphax) * 2) - 1\n",
    "            grady2_s = (self.sigmoid(grady2 * alphay) * 2) - 1\n",
    "            gradx_loss += self._all_comb(gradx1_s, gradx2_s)\n",
    "            grady_loss += self._all_comb(grady1_s, grady2_s)\n",
    "            img1 = self.avg_pool(img1)\n",
    "            img2 = self.avg_pool(img2)\n",
    "        return gradx_loss, grady_loss\n",
    "\n",
    "    def _all_comb(self, grad1_s, grad2_s):\n",
    "        v = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                v.append(torch.mean(((grad1_s[:, j, :, :] ** 2) * (grad2_s[:, i, :, :] ** 2))) ** 0.25)\n",
    "        return v\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        gradx_loss, grady_loss = self.get_gradients(img1, img2)\n",
    "        loss_gradxy = sum(gradx_loss) / (self.level * 9) + sum(grady_loss) / (self.level * 9)\n",
    "        return loss_gradxy / 2.0\n",
    "\n",
    "    def compute_gradient(self, img):\n",
    "        gradx = img[:, :, 1:, :] - img[:, :, :-1, :]\n",
    "        grady = img[:, :, :, 1:] - img[:, :, :, :-1]\n",
    "        return gradx, grady\n",
    "    \n",
    "def gradient(pred):\n",
    "    D_dy      = pred[:, :, 1:] - pred[:, :, :-1]\n",
    "    D_dx      = pred[:, :, :, 1:] - pred[:, :, :, :-1]\n",
    "    return D_dx, D_dy\n",
    "\n",
    "def smooth_loss(pred_map):\n",
    "    dx, dy   = gradient(pred_map)\n",
    "    dx2, dxdy= gradient(dx)\n",
    "    dydx, dy2= gradient(dy)\n",
    "    loss     =  (dx2.abs().mean()  + dxdy.abs().mean()+ \n",
    "                 dydx.abs().mean() + dy2.abs().mean())\n",
    "    return loss\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    gray = 0.2989*rgb[:,:,0:1,:] + \\\n",
    "    \t   0.5870*rgb[:,:,1:2,:] + \\\n",
    "    \t   0.1140*rgb[:,:,2:3,:]\n",
    "    return gray\n",
    "\n",
    "def demo(args,\n",
    "         dle_net,\n",
    "         optimizer_dle_net,\n",
    "         inputs):\n",
    "    \n",
    "    dle_net.train()\n",
    "\n",
    "    img_in    = Variable(torch.FloatTensor(inputs['img_in'])).cuda()\n",
    "    optimizer_dle_net.zero_grad()\n",
    "\n",
    "    le_pred = dle_net(img_in)\n",
    "    dle_pred= img_in + le_pred\n",
    "\n",
    "    lambda_cc         = 1.0 \n",
    "    dle_pred_cc       = torch.mean(dle_pred, dim=1, keepdims=True)\n",
    "    cc_loss           = (F.l1_loss(dle_pred[:, 0:1, :, :], dle_pred_cc) + \\\n",
    "                         F.l1_loss(dle_pred[:, 1:2, :, :], dle_pred_cc) + \\\n",
    "                         F.l1_loss(dle_pred[:, 2:3, :, :], dle_pred_cc))*(1/3) ##Color Constancy Loss\n",
    "    \n",
    "    lambda_recon        = 1.0\n",
    "    recon_loss          = F.l1_loss(dle_pred, img_in)                         \n",
    "    \n",
    "    lambda_excl        = 0.01\n",
    "    data_type          = torch.cuda.FloatTensor\n",
    "    excl_loss          = ExclusionLoss().type(data_type)                      \n",
    "\n",
    "    lambda_smooth       = 1.0 \n",
    "    le_smooth_loss      = smooth_loss(le_pred)\n",
    "\n",
    "    loss = lambda_recon*recon_loss + \\\n",
    "           lambda_cc*cc_loss\n",
    "    loss += lambda_excl * excl_loss(dle_pred, le_pred)\n",
    "    loss += lambda_smooth*le_smooth_loss\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer_dle_net.step()\n",
    "\n",
    "    imgs_dict   = {}\n",
    "    imgs_dict['dle_pred'] = dle_pred.detach().cpu()\n",
    "    return imgs_dict\n",
    "\n",
    "class Arguments:\n",
    "    def __init__(self):\n",
    "        self.out_dir = './light-effects-output/'\n",
    "        self.data_dir = './light-effects/'\n",
    "        self.load_model = None\n",
    "        self.load_size = \"Resize\"\n",
    "        self.crop_size = \"[512, 512]\"\n",
    "        self.iters = 60\n",
    "        self.learning_rate = 1e-4\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "args.imgin_dir = args.data_dir\n",
    "args.use_gray  = False\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "args.imgs_dir = args.out_dir\n",
    "if not os.path.exists(args.imgs_dir):\n",
    "    os.makedirs(args.imgs_dir)\n",
    "\n",
    "if args.use_gray:\n",
    "    channels = 1\n",
    "else:\n",
    "    channels = 3\n",
    "dle_net = Net(input_nc=channels, output_nc=channels)\n",
    "dle_net = nn.DataParallel(dle_net).cuda()\n",
    "\n",
    "if args.load_model is not None:\n",
    "    dle_net_ckpt_file = args.load_model\n",
    "    dle_net.load_state_dict(torch.load(dle_net_ckpt_file)['state_dict'])\n",
    "\n",
    "optimizer_dle_net = optim.Adam(dle_net.parameters(), \n",
    "                                  lr=args.learning_rate, \n",
    "                                  betas=(0.9, 0.999))\n",
    "\n",
    "in_filenames = sorted([join(args.imgin_dir, x) for x in listdir(args.imgin_dir) if is_image_file(x)])\n",
    "\n",
    "for in_filename in in_filenames:\n",
    "    img_name = basename(in_filename)\n",
    "    print('img_name',in_filename)\n",
    "    img  = Image.open(in_filename).convert('RGB')\n",
    "    w, h = img.size\n",
    "\n",
    "    if h != 512 or w != 512:\n",
    "        img = img.resize([512, 512], Image.LANCZOS)  \n",
    "\n",
    "    da_list     = sorted([(args.imgin_dir + file) for file in os.listdir(args.imgin_dir) if file == img_name])\n",
    "    demo_list   = da_list\n",
    "    demo_list   = demo_list*args.iters\n",
    "\n",
    "    Dele_Loader = torch.utils.data.DataLoader(DA.loadImgs(args, \n",
    "                                                           demo_list,\n",
    "                                                           mode='demo'),\n",
    "                                               batch_size  = 1, \n",
    "                                               shuffle     = True, \n",
    "                                               num_workers = 16, \n",
    "                                               drop_last   = False)\n",
    "    count_idx = 0\n",
    "    tbar = Dele_Loader\n",
    "    for batch_idx, inputs in enumerate(tbar):\n",
    "        count_idx = count_idx + 1\n",
    "        imgs_dict = demo(args,\n",
    "                          dle_net, \n",
    "                          optimizer_dle_net,\n",
    "                          inputs)\n",
    "\n",
    "        if (count_idx%60 == 0):\n",
    "            inout = os.path.join(args.imgs_dir, img_name[:-4]+'_in_out')\n",
    "            out   = os.path.join(args.imgs_dir, img_name[:-4]+'_out')\n",
    "            save_img   = torch.cat((inputs['img_in'][0, :, :, :],\n",
    "                                    imgs_dict['dle_pred'][0, :, :, :]), dim=2)\n",
    "            in_img  = inputs['img_in'][0, :, :, :]\n",
    "            out_img = imgs_dict['dle_pred'][0, :, :, :]\n",
    "            test = in_img.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()\n",
    "            test = Image.fromarray(test)\n",
    "            test = test.resize([w, h], Image.LANCZOS)\n",
    "            result = out_img.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()\n",
    "            result = Image.fromarray(result)\n",
    "            result = result.resize([w, h], Image.LANCZOS)\n",
    "\n",
    "            vutils.save_image(save_img, inout+'.png')\n",
    "            vutils.save_image(out_img, out+'.png')\n",
    "            \n",
    "            in_img_np = in_img.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()\n",
    "            out_img_np = out_img.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            axs[0].imshow(in_img_np)\n",
    "            axs[0].set_title('Input')\n",
    "            axs[0].axis('off')\n",
    "            axs[1].imshow(out_img_np)\n",
    "            axs[1].set_title('Output')\n",
    "            axs[1].axis('off')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
