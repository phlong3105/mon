#!/usr/bin/env python
# -*- coding: utf-8 -*-

# https://github.com/AndersonYong/URetinex-Net

from __future__ import annotations

import argparse
import time

import torch.nn as nn
import torchvision.transforms as transforms

import mon
from network.decom import Decom
from network.Math_Module import P, Q
from utils import *

console = mon.console

"""
As different illumination adjustment ratio will cause different enhanced
results. Certainly you can tune the ratio yourself to get the best results. To
get a better result, we use the illumination of normal light image to adaptively
generate ratio. Noted that KinD and KinD++ also use ratio to guide the
illumination adjustment, for fair comparison, the ratio of their methods also
generated by the illumination of normal light image.
"""


def one2three(x):
    return torch.cat([x, x, x], dim=1).to(x)


class Inference(nn.Module):
    
    def __init__(self, opts):
        super().__init__()
        self.opts = opts
        # Loading decomposition model
        self.model_Decom_low  = Decom()
        self.model_Decom_high = Decom()
        self.model_Decom_low  = load_initialize(self.model_Decom_low, self.opts.Decom_model_low_weights)
        self.model_Decom_high = load_initialize(self.model_Decom_high, self.opts.Decom_model_high_weights)
        # Loading R; old_model_opts; and L model
        self.unfolding_opts, self.model_R, self.model_L = load_unfolding(self.opts.unfolding_model_weights)
        # Loading adjustment model
        self.adjust_model     = load_adjustment(self.opts.adjust_model_weights)
        self.P = P()
        self.Q = Q()
        transform = [
            transforms.ToTensor(),
        ]
        self.transform = transforms.Compose(transform)
        # print(self.model_Decom_low)
        # print(self.model_R)
        # print(self.model_L)
        # print(self.adjust_model)
        # time.sleep(8)
        
    def get_ratio(self, high_l, low_l):
        ratio     = (low_l / (high_l + 0.0001)).mean()
        low_ratio = torch.ones(high_l.shape).cuda() * (1/(ratio + 0.0001))
        return low_ratio
    
    def unfolding(self, input_low_img):
        for t in range(self.unfolding_opts.round):      
            if t == 0:  # Initialize R0, L0
                P, Q = self.model_Decom_low(input_low_img)
            else:  # Update P and Q
                w_p  = (self.unfolding_opts.gamma + self.unfolding_opts.Roffset * t)
                w_q  = (self.unfolding_opts.lamda + self.unfolding_opts.Loffset * t)
                P    = self.P(I=input_low_img, Q=Q, R=R, gamma=w_p)
                Q    = self.Q(I=input_low_img, P=P, L=L, lamda=w_q)
            R = self.model_R(r=P, l=Q)
            L = self.model_L(l=Q)
        return R, L
    
    def illumination_adjust(self, L, ratio):
        ratio = torch.ones(L.shape).cuda() * ratio
        return self.adjust_model(l=L, alpha=ratio)
    
    def forward(self, input_low_img, input_high_img):
        if torch.cuda.is_available():
            input_low_img  = input_low_img.cuda()
            input_high_img = input_high_img.cuda()
        with torch.no_grad():
            start_time = time.time()
            R, L       = self.unfolding(input_low_img)
            # The ratio is calculated using the decomposed normal illumination
            _, high_L  = self.model_Decom_high(input_high_img)
            ratio      = self.get_ratio(high_L, L)
            High_L     = self.illumination_adjust(L, ratio)
            I_enhance  = High_L * R
            run_time   = (time.time() - start_time)
        return I_enhance, run_time

    def evaluate(self):
        low_image_paths = list(self.opts.data_low.rglob("*"))
        low_image_paths = [path for path in low_image_paths if path.is_image_file()]
        with mon.get_progress_bar() as pbar:
            for _, low_image_path in pbar.track(
                sequence    = enumerate(low_image_paths),
                total       = len(low_image_paths),
                description = f"[bright_yellow] Inferring"
            ):
                high_image_path   = self.opts.data_high / low_image_path.name
                low_image         = self.transform(Image.open(low_image_path)).unsqueeze(0)
                high_image        = self.transform(Image.open(high_image_path)).unsqueeze(0)
                enhance, run_time = self.forward(low_image, high_image)
                result_path       = self.opts.output_dir / low_image_path.name
                torchvision.utils.save_image(enhance, str(result_path))
            """
            if not os.path.exists(self.opts.output):
                os.makedirs(self.opts.output)
            save_path = os.path.join(self.opts.output, file_name.replace(name, "%s_URetinexNet"%(name)))
            np_save_TensorImg(enhance, save_path)
            print("================================= time for %s: %f============================"%(file_name, p_time))
            """
            return enhance, run_time
            
    
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Configure")
    parser.add_argument("--data-low",                 type=str, default="./test_daat/LOLdataset/eval15/low")
    parser.add_argument("--data-high",                type=str, default="./test_data/LOLdataset/eval15/high")
    parser.add_argument("--decom-model-low-weights",  type=str, default="./ckpt/init_low.pth")
    parser.add_argument("--decom-model-high-weights", type=str, default="./ckpt/init_high.pth")
    parser.add_argument("--unfolding-model-weights",  type=str, default="./ckpt/unfolding.pth")
    parser.add_argument("--adjust-model-weights",     type=str, default="./ckpt/L_adjust.pth")
    parser.add_argument("--gpu",                      type=int, default=0)
    parser.add_argument("--output-dir",               type=str, default="./demo/output/LOL")
    args = parser.parse_args()

    os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu)
    
    args.data_low   = mon.Path(args.data_low)
    args.data_high  = mon.Path(args.data_high)
    args.output_dir = mon.Path(args.output_dir)
    args.output_dir.mkdir(parents=True, exist_ok=True)

    model = Inference(args).cuda()
    model.evaluate()
