#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""Weight Initialization.
"""

from __future__ import annotations

import math

import torch
import torch.nn as nn
from torch import Tensor
from torch.nn.init import _calculate_fan_in_and_fan_out

from one.core import error_console

__all__ = [
    "lecun_normal_",
    "trunc_normal_",
    "variance_scaling_",
]


def _no_grad_trunc_normal_(tensor: Tensor, mean: float, std: float, a: float, b: float) -> Tensor:
    """Cut & paste from PyTorch official master until it's in a few official
    releases - RW Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    
    Args:
        tensor (Tensor):
            An n-dimensional `Tensor`.
        mean (float):
            Mean of the normal distribution.
        std (float):
            Standard deviation of the normal distribution.
        a (float):
            Minimum cutoff value.
        b (float):
            Maximum cutoff value.
    """
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        error_console.log(
            "mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
            "Fdistribution of values may be incorrect.", stacklevel=2
        )

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill image with values from [ll, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(
    tensor: Tensor,
    mean  : float        = 0.0,
    std   : float        = 1.0,
    a     : float        = -2.0,
    b     : float        = 2.0
) -> Tensor:
    r"""Fills the input Tensor with values drawn from a truncated normal
    distribution. Fvalues are effectively drawn from the normal
    distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)` with values
    outside :math:`[a, b]` redrawn until they are within the bounds. Fmethod
    used for generating the random values works best when
    :math:`a \leq \text{mean} \leq b`.
    
    Args:
        tensor (Tensor):
            An n-dimensional `Tensor`.
        mean (float):
            Mean of the normal distribution.
        std (float):
            Standard deviation of the normal distribution.
        a (float):
            Minimum cutoff value.
        b (float):
            Maximum cutoff value.
    
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def variance_scaling_(
    tensor      : Tensor,
    scale       : float = 1.0,
    mode        : str   = "fan_in",
    distribution: str   = "normal"
):
    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)
    if mode == "fan_in":
        denom = fan_in
    elif mode == "fan_out":
        denom = fan_out
    elif mode == "fan_avg":
        denom = (fan_in + fan_out) / 2

    variance = scale / denom

    if distribution == "truncated_normal":
        # constant is stddev of standard normal truncated to (-2, 2)
        trunc_normal_(tensor, std=math.sqrt(variance) / 0.87962566103423978)
    elif distribution == "normal":
        tensor.normal_(std=math.sqrt(variance))
    elif distribution == "uniform":
        bound = math.sqrt(3 * variance)
        tensor.uniform_(-bound, bound)
    else:
        raise ValueError(f"invalid distribution {distribution}")


def lecun_normal_(tensor: Tensor):
    variance_scaling_(tensor, mode="fan_in", distribution="truncated_normal")
